  0%|                                                                                                      | 0/1944 [00:00<?, ?it/s]/workspace/storage/finetune/envt/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(





































































































































































































































































































































































































































































































 25%|██████████████████████                                                                  | 486/1944 [2:30:14<7:24:54, 18.31s/it]Saving model checkpoint to ../training_output/tmp-checkpoint-486
Configuration saved in ../training_output/tmp-checkpoint-486/config.json
Configuration saved in ../training_output/tmp-checkpoint-486/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ../training_output/tmp-checkpoint-486/model.safetensors.index.json.
tokenizer config file saved in ../training_output/tmp-checkpoint-486/tokenizer_config.json
Special tokens file saved in ../training_output/tmp-checkpoint-486/special_tokens_map.json
added tokens file saved in ../training_output/tmp-checkpoint-486/added_tokens.json
/workspace/storage/finetune/envt/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-20 05:08:13,971] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step486 is about to be saved!
[2024-04-20 05:08:13,980] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../training_output/tmp-checkpoint-486/global_step486/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-20 05:08:13,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../training_output/tmp-checkpoint-486/global_step486/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-20 05:08:13,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../training_output/tmp-checkpoint-486/global_step486/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-20 05:08:13,995] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../training_output/tmp-checkpoint-486/global_step486/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-20 05:08:41,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../training_output/tmp-checkpoint-486/global_step486/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-20 05:08:41,255] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ../training_output/tmp-checkpoint-486/global_step486/zero_pp_rank_0_mp_rank_00_optim_states.pt
/workspace/storage/finetune/envt/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
[2024-04-20 05:08:42,897] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step486 is ready now!













 26%|██████████████████████▋                                                                 | 500/1944 [2:35:21<7:35:10, 18.91s/it]
























































































































































































































































































































































































































































































 50%|████████████████████████████████████████████                                            | 972/1944 [5:02:25<4:46:03, 17.66s/it]Saving model checkpoint to ../training_output/tmp-checkpoint-972
Configuration saved in ../training_output/tmp-checkpoint-972/config.json
Configuration saved in ../training_output/tmp-checkpoint-972/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ../training_output/tmp-checkpoint-972/model.safetensors.index.json.
tokenizer config file saved in ../training_output/tmp-checkpoint-972/tokenizer_config.json
Special tokens file saved in ../training_output/tmp-checkpoint-972/special_tokens_map.json
added tokens file saved in ../training_output/tmp-checkpoint-972/added_tokens.json
/workspace/storage/finetune/envt/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-20 07:40:26,451] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step972 is about to be saved!
[2024-04-20 07:40:26,459] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../training_output/tmp-checkpoint-972/global_step972/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-20 07:40:26,459] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../training_output/tmp-checkpoint-972/global_step972/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-20 07:40:26,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../training_output/tmp-checkpoint-972/global_step972/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-20 07:40:26,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../training_output/tmp-checkpoint-972/global_step972/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-20 07:40:56,627] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../training_output/tmp-checkpoint-972/global_step972/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-20 07:40:56,628] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ../training_output/tmp-checkpoint-972/global_step972/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-20 07:40:56,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step972 is ready now!
Deleting older checkpoint [../training_output/checkpoint-486] due to args.save_total_limit
/workspace/storage/finetune/envt/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(



























 51%|████████████████████████████████████████████▊                                          | 1000/1944 [5:12:02<4:53:48, 18.67s/it]










































































































































































































































































































































































































































































 75%|█████████████████████████████████████████████████████████████████▎                     | 1458/1944 [7:31:04<2:25:20, 17.94s/it]Saving model checkpoint to ../training_output/tmp-checkpoint-1458
Configuration saved in ../training_output/tmp-checkpoint-1458/config.json
Configuration saved in ../training_output/tmp-checkpoint-1458/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ../training_output/tmp-checkpoint-1458/model.safetensors.index.json.
tokenizer config file saved in ../training_output/tmp-checkpoint-1458/tokenizer_config.json
Special tokens file saved in ../training_output/tmp-checkpoint-1458/special_tokens_map.json
added tokens file saved in ../training_output/tmp-checkpoint-1458/added_tokens.json
/workspace/storage/finetune/envt/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-20 10:09:03,691] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1458 is about to be saved!
[2024-04-20 10:09:03,700] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../training_output/tmp-checkpoint-1458/global_step1458/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-20 10:09:03,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../training_output/tmp-checkpoint-1458/global_step1458/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-20 10:09:03,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../training_output/tmp-checkpoint-1458/global_step1458/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-20 10:09:03,717] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../training_output/tmp-checkpoint-1458/global_step1458/zero_pp_rank_0_mp_rank_00_optim_states.pt...
Deleting older checkpoint [../training_output/checkpoint-972] due to args.save_total_limit
[2024-04-20 10:09:32,386] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../training_output/tmp-checkpoint-1458/global_step1458/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-20 10:09:32,387] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ../training_output/tmp-checkpoint-1458/global_step1458/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-20 10:09:32,393] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1458 is ready now!
/workspace/storage/finetune/envt/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(








































 77%|███████████████████████████████████████████████████████████████████                    | 1499/1944 [7:44:46<2:19:48, 18.85s/it]





























































































































































































































































































































































































































































100%|████████████████████████████████████████████████████████████████████████████████████████| 1944/1944 [10:02:01<00:00, 17.78s/it]Saving model checkpoint to ../training_output/tmp-checkpoint-1944
Configuration saved in ../training_output/tmp-checkpoint-1944/config.json
Configuration saved in ../training_output/tmp-checkpoint-1944/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ../training_output/tmp-checkpoint-1944/model.safetensors.index.json.
tokenizer config file saved in ../training_output/tmp-checkpoint-1944/tokenizer_config.json
Special tokens file saved in ../training_output/tmp-checkpoint-1944/special_tokens_map.json
added tokens file saved in ../training_output/tmp-checkpoint-1944/added_tokens.json
/workspace/storage/finetune/envt/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-04-20 12:40:01,505] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1944 is about to be saved!
[2024-04-20 12:40:01,514] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../training_output/tmp-checkpoint-1944/global_step1944/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-04-20 12:40:01,514] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../training_output/tmp-checkpoint-1944/global_step1944/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-04-20 12:40:01,527] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../training_output/tmp-checkpoint-1944/global_step1944/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-04-20 12:40:01,528] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../training_output/tmp-checkpoint-1944/global_step1944/zero_pp_rank_0_mp_rank_00_optim_states.pt...
Deleting older checkpoint [../training_output/checkpoint-1458] due to args.save_total_limit
[2024-04-20 12:40:31,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../training_output/tmp-checkpoint-1944/global_step1944/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-20 12:40:31,445] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ../training_output/tmp-checkpoint-1944/global_step1944/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-20 12:40:31,452] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1944 is ready now!
{'train_runtime': 36178.7082, 'train_samples_per_second': 1.719, 'train_steps_per_second': 0.054, 'train_loss': 0.12312812962159207, 'epoch': 4.0}
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|████████████████████████████████████████████████████████████████████████████████████████| 1944/1944 [10:02:57<00:00, 18.61s/it]
Saving model checkpoint to ../training_output
Configuration saved in ../training_output/config.json
Configuration saved in ../training_output/generation_config.json
The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ../training_output/model.safetensors.index.json.
tokenizer config file saved in ../training_output/tokenizer_config.json
Special tokens file saved in ../training_output/special_tokens_map.json
added tokens file saved in ../training_output/added_tokens.json