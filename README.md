

# Instruction-Based Dataset Generation and Model Fine-Tuning in LLMs

Goal: The objective of this assignment is to explore the process of generating an instruction-based dataset for model training in Natural Language Processing (NLP). Additionally, students will fine-tune a pre-trained model using the newly created instruction-based dataset and compare its performance with the original instructions. Moreover, they will test how the model behaves before and after training with general-purpose instructions in which the model was originally trained.

Summary: The instruction dataset consists of decompiled code with comments regarding whether a vulnerability exists or not. For nonvulnerable functions the comments field is blank. For vulnerable functions, the comment field consists of comments from the code explaining the vulnerability. We analyze the models by asking the model to describe the vulnerability if exists.

#### Requirements

Instructions to install libraries using *requirements.txt* file.

```shell
pip install -r requirements.txt
```
#### Datasets 
Dataset splits for test and training and the CSV files can be downloaded from this google drive link: https://drive.google.com/file/d/1rg8L7qxcs1qwg58CTxbyDV7Z9BTQZAbH/view?usp=sharing
### Tasks:
1. Instruction-Based Dataset Generation (50 pts):
a.Select any non-instruction-based dataset from a previously available source in NLP.
- The datasets chosen were FORM AI and MVD datasets. Both datasets contain C programs of vulnerable and nonvulnerable code with their we vulnerability classification and comments on the vulnerabilities. Both datasets were combined and randomly selected 35k records. 
FormAI: The FormAI dataset, generated by the GPT-3.5-turbo model. 
MVD: The MVD dataset, sourced from the Software Assurance Reference Dataset (SARD) and the National Vulnerability Database (NVD).

- The final file is *MVD_35K_prompt.csv*

- To create the Test and Train splits we use *split_dataset.py*. The splits are under the folder *MVD_35K_prompt_split*

2. Used ChatGPT LLM to create two sets of clear and concise instructions for each task represented in the dataset. 

- The script to add the prompts to the dataset is *add_promp_MVD.py* 
- The instructions, there are two types of instructions, one to identify the CWE vulnerability and another to describe the vulnerability. For each type, I used ChatGPT to create 20 different instructions. Then, for each record in our dataset, I randomly assigned one of the instructions, alternating between both types. For this assignment, we only evaluate the described instructions as given scores do better with text. 
- The final file is *MVD_35K_prompt.csv*

3. Convert the original dataset into an instruction-based dataset by appending the generated instructions to each data instance (each row of the dataset).

- The base model is codeLLama-7b-hf which dont make their original training dataset available. So for this task, we used the Alpaca dataset to append 15K rows to our instruct dataset created in the last step. 

- The file is *MVD_alpaca.csv*
- The splits are under the folder *MVD_alpaca_split
- The script used to append the records is *add_records_alpaca.py

4. Model Fine-Tuning (15 pts):

b. Fine-tune the same pre-trained LLM using the instruction-based dataset generated in Task 1. Save the model.

- The code for finetuning the LLM is under *LLM_finetune*. To run the finetuning you will run the command *bash run_training.sh*
- The weights for the CodeLLama finetune with our instruct dataset is under */assignment 3/LLM_finetune/MVD_35K*
c. Again, fine-tune the original pre-trained LLM by combining your instruction dataset and the original dataset the model was initially trained on. Save your model.
- The weights for the CodeLLama finetune with our instruct dataset and Alpaca dataset is under */assignment 3/LLM_finetune/MVD_35K_Alpaca*


5. Comparison (35 pts):

Original Model: codellama/CodeLlama-7b-hf

a.Evaluate the saved model from 2.b and 2.b an on your proposed dataset.

| Dataset | Model                 | Bleu | Rouge-L | Bert Score        | 
|---------|-----------------------|------|---------|-------------------|
|mvd decom| Code Llama Base Model |0.0043|0.1273   |0.7858,0.8507,0.816|           
|mvd decom| Fine-tuned at 2. b    |0.1460|0.2643   |0.8854,0.8433,0.863|           
|mvd/alpac| Fine-tuned at 2. c    |0.2299|0.3039   |0.8939,0.8634,0.877|           
 
Per our results, the original CodeLlama had pretty low scores, which probably is due to codeLlama not being able to understand the decompiled code. After the training we saw the scores increase, especially the Bert score. For the Bleu and Rouge score, the low score might be related to not having good comments explaining and describing the vulnerabilities and the non-vulnerable records. The model whose scores were a little bit higher were with the mvd and alpaca dataset, which could indicate that merging these two, provides the model with a better understanding of the new task and also not forgetting their original training.  

b.Create 10 instructions completely out-of-sample from your dataset. 

Results for MVD-Alpaca: */assignment 3/result_MVD_Alpaca_10Samples.json*

Results for MVD: */assignment 3 result_MVD_10Samples.json*

Script: */assignment 3/infer_10instoutsample.py*

Following the evaluation results from the last section, the obtained results with the 10 out of sample instructions seem to follow the evaluation in which the mvd-alpaca dataset got more descriptions correct. However, my descriptions are still not consistent due to needing more work to get better descriptions in my two datasets for vulnerable and nonvulnerable functions. Right now, only my vulnerable functions have some comments that describe the vulnerability but the nonvulnerable are blank. 

### References

- Norbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag, Lucas C. Cordeiro, and Vasileios Mavroeidis. 2023. The FormAI Dataset: Generative AI in Software Security through the Lens of Formal Verification. In Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2023). Association for Computing Machinery, New York, NY, USA, 33â€“43. https://doi.org/10.1145/3617555.3617874
- muVulDeePecker: https://github.com/muVulDeePecker/muVulDeePecker

## License
As a free open-source implementation, our repository is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. All other warranties including, but not limited to, merchantability and fitness for purpose, whether express, implied, or arising by operation of law, course of dealing, or trade usage are hereby disclaimed. I believe that the programs compute what I claim they compute, but I do not guarantee this. The programs may be poorly and inconsistently documented and may contain undocumented components, features or modifications. I make no guarantee that these programs will be suitable for any application.

